{"cells":[{"cell_type":"markdown","source":["**Install Neccessary Dependencies**"],"metadata":{"id":"jmsD0rh5QjEY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWgQmZUCsTlc"},"outputs":[],"source":["try:\n","    import torch_geometric\n","except ImportError:\n","    !pip install torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2zm8GeW0875"},"outputs":[],"source":["!pip install umap-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaQ05baA4ydp"},"outputs":[],"source":["pip install --upgrade \"numba==0.59.1\" \"llvmlite==0.42.0\"    # if neccessary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lmffwb5Atpmh"},"outputs":[],"source":["!pip install scanpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a9f4AsQucFr"},"outputs":[],"source":["!pip install scikit-misc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LyhCWzjFNyXw"},"outputs":[],"source":["!pip install python-igraph leidenalg"]},{"cell_type":"markdown","source":["**Import neccessary libraries**"],"metadata":{"id":"DUDl0vGhR4LP"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import scanpy as sc\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n","from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score, adjusted_mutual_info_score, confusion_matrix\n","from sklearn.decomposition import PCA\n","import umap\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy.stats import ttest_ind, ranksums\n","from sklearn.cluster import KMeans\n","from sklearn.neighbors import NearestNeighbors"],"metadata":{"id":"ZKkbtvueR0QU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Pathway Aware GNN Framework**"],"metadata":{"id":"eFxcpVqJRX-5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jTZqYbiniyu"},"outputs":[],"source":["# ---- DEVICE SETUP ----\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"[INFO] Using device:\", device)\n","\n","# ---- 1. SCTransform-like Preprocessing----\n","def preprocess_sctransform(adata, regress_vars=None, hvg_n=2000):\n","    print(\"[INFO] Preprocessing with SCTransform-like workflow...\")\n","    sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=hvg_n)\n","    adata = adata[:, adata.var['highly_variable']]\n","    if regress_vars is not None:\n","        sc.pp.regress_out(adata, regress_vars)\n","    sc.pp.scale(adata)\n","    print(\"[INFO] Preprocessing completed.\")\n","    return adata\n","\n","def construct_pathway_bipartite_graphs(\n","    adata,\n","    pathway_genes,\n","    k=5,\n","    min_expression=0.25,          # absolute floor for expression to count as an edge\n","    gene_percentile=75,           # per-gene percentile threshold\n","    min_genes_required=5,         # skip pathways with < this many genes in data\n","    min_edges_per_gene=2,         # require at least this many edges per gene on avg\n","    fallback_topk=True            # if too sparse, rescue with top-k per cell\n","):\n","    \"\"\"\n","    Build bipartite graphs (genes ↔ cells) per pathway using selective edges:\n","      - add an edge gene↔cell only if expression > max(gene-specific percentile, min_expression)\n","      - if graph is too sparse, optionally fall back to top-k per cell for that pathway\n","    \"\"\"\n","    print(\"[INFO] Building bipartite graphs (thresholded + optional top-k fallback)...\")\n","    expr = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n","    graphs = {}\n","\n","    for pathway_name, genes in pathway_genes.items():\n","        available_genes = [g for g in genes if g in expr.columns]\n","        if len(available_genes) < min_genes_required:\n","            continue\n","\n","        G = nx.Graph()\n","        G.add_nodes_from(available_genes, bipartite=0, node_type='gene')\n","        G.add_nodes_from(expr.index, bipartite=1, node_type='cell')\n","\n","        # ---- selective edges by threshold ----\n","        edges_added = 0\n","        for gene in available_genes:\n","            gene_expr = expr[gene]\n","            # robust per-gene threshold on non-zero values\n","            if (gene_expr > 0).sum() > 0:\n","                per_gene_thr = np.percentile(gene_expr[gene_expr > 0], gene_percentile)\n","            else:\n","                per_gene_thr = float('inf')  # no non-zero expression; skip adding edges\n","\n","            thr = max(per_gene_thr, min_expression)\n","\n","            # add edges for cells where expression is \"meaningfully high\"\n","            high_cells = gene_expr.index[gene_expr > thr]\n","            for cell in high_cells:\n","                val = gene_expr.loc[cell]\n","                # Handle both scalar and non-scalar cases\n","                val_scalar = val.iloc[0] if hasattr(val, 'iloc') else val\n","                if val_scalar > 0:\n","                    # weight can be raw or normalized; keep raw for interpretability\n","                    G.add_edge(gene, cell, weight=float(val_scalar))\n","                    edges_added += 1\n","\n","        # ---- density check + fallback ----\n","        needed_edges = len(available_genes) * min_edges_per_gene\n","        if edges_added < needed_edges and fallback_topk:\n","            # rescue sparsity by adding top-k per cell\n","            pathway_expr = expr[available_genes].to_numpy()\n","            # normalize per cell to avoid scale dominance\n","            row_max = pathway_expr.max(axis=1, keepdims=True)\n","            row_max[row_max == 0] = 1.0\n","            norm_expr = pathway_expr / row_max\n","\n","            for i, cell in enumerate(expr.index):\n","                cell_vec = norm_expr[i]\n","                # choose top-k genes among available genes (skip zeros)\n","                k_eff = min(k, (cell_vec > 0).sum()) if (cell_vec > 0).any() else 0\n","                if k_eff > 0:\n","                    top_idx = np.argpartition(cell_vec, -k_eff)[-k_eff:]\n","                    for gi in top_idx:\n","                        gene = available_genes[gi]\n","                        if not G.has_edge(gene, cell):\n","                            raw_val = pathway_expr[i, gi]  #raw_val = expr.at[cell, gene]\n","                            if raw_val > 0:\n","                                G.add_edge(gene, cell, weight=float(raw_val))\n","                                edges_added += 1\n","\n","        if edges_added >= needed_edges:\n","            graphs[pathway_name] = G\n","            print(f\"  [INFO] {pathway_name}: {len(available_genes)} genes, {edges_added} edges\")\n","        else:\n","            # too sparse even after fallback; drop it\n","            pass\n","\n","    print(f\"[INFO] Graph building completed. Kept pathways: {len(graphs)}\")\n","    return graphs\n","\n","# ---- 3. Node Features: [mean, std, degree, type] + PCA profile (cells) + pathway membership (genes) ----\n","def node_features_bipartite_enhanced(G, expr, pathway_genes=None, pathway=None, n_pca=5):\n","    genes = [n for n, d in G.nodes(data=True) if d['node_type'] == 'gene']\n","    cells = [n for n, d in G.nodes(data=True) if d['node_type'] == 'cell']\n","\n","    # PCA for cells\n","    if len(genes) > 1 and len(cells) > 1:\n","        pca = PCA(n_components=min(n_pca, len(genes)))\n","        cell_pca = pca.fit_transform(expr.loc[cells, genes])\n","        pca_dim = cell_pca.shape[1]\n","    else:\n","        cell_pca = np.zeros((len(cells), n_pca))\n","        pca_dim = n_pca\n","\n","    features = []\n","    # --- Genes ---\n","    for gene in genes:\n","        vals = expr[gene] if gene in expr.columns else np.zeros(len(cells))\n","        pathway_score = len(pathway_genes[pathway]) if pathway_genes and pathway else 0\n","        vec = [np.mean(vals), np.std(vals), G.degree(gene), 1, 0]  # base features\n","        vec += [pathway_score]                                     # pathway info\n","        vec += [0.0] * pca_dim                                     # pad PCA slots\n","        features.append(vec)\n","\n","    # --- Cells ---\n","    for i, cell in enumerate(cells):\n","        vals = expr.loc[cell, genes] if cell in expr.index else np.zeros(len(genes))\n","        vec = [np.mean(vals), np.std(vals), G.degree(cell), 0, 1]  # base features\n","        vec += [0.0]                                               # dummy pathway score slot\n","        vec += list(cell_pca[i])                                   # real PCA profile\n","        features.append(vec)\n","\n","    x = np.array(features, dtype=np.float32)\n","    return x, genes, cells\n","\n","# ---- 4. Different Models ----\n","class UnifiedGNN(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, out_dim, gnn_type='gcn', heads=2):\n","        super().__init__()\n","        self.gnn_type = gnn_type.lower()\n","        self.heads = heads\n","        if self.gnn_type == 'gcn':\n","            self.conv1 = GCNConv(in_dim, hidden_dim)\n","            self.conv2 = GCNConv(hidden_dim, out_dim)\n","        elif self.gnn_type == 'sage':\n","            self.conv1 = SAGEConv(in_dim, hidden_dim)\n","            self.conv2 = SAGEConv(hidden_dim, out_dim)\n","        elif self.gnn_type == 'gat':\n","            self.conv1 = GATConv(in_dim, hidden_dim, heads=heads, concat=False)\n","            self.conv2 = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n","        else:\n","            raise ValueError(\"gnn_type must be one of: 'gcn', 'sage', 'gat'\")\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","# ---- 5. Contrastive Loss (with dropout-based augmentation) ----\n","def contrastive_loss_dropout(cell_emb, drop_prob=0.2, temperature=0.5):\n","    # Dropout-based augmentation\n","    mask = (torch.rand_like(cell_emb) > drop_prob).float()\n","    cell_emb_aug = cell_emb * mask\n","    N = cell_emb.shape[0]\n","    z1 = F.normalize(cell_emb, dim=1)\n","    z2 = F.normalize(cell_emb_aug, dim=1)\n","    representations = torch.cat([z1, z2], dim=0)\n","    similarity_matrix = torch.matmul(representations, representations.T)\n","    mask_eye = torch.eye(2*N, dtype=torch.bool, device=cell_emb.device)\n","    similarity_matrix = similarity_matrix.masked_fill(mask_eye, -9e15)\n","    positives = torch.cat([torch.diag(similarity_matrix, N), torch.diag(similarity_matrix, -N)], dim=0)\n","    negatives = similarity_matrix[~mask_eye].view(2*N, -1)\n","    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1) / temperature\n","    labels = torch.zeros(2*N, dtype=torch.long, device=cell_emb.device)\n","    loss = F.cross_entropy(logits, labels)\n","    return loss.item(), loss\n","\n","# ---- 6. Train GNNs only (early stopping) ----\n","def train_pathway_gnns(graphs, expr, pathway_genes, model_type='sage', hidden_dim=32, out_dim=16,\n","                       epochs=100, early_stop_patience=15, early_stop_delta=1e-4, device=\"cuda\"):\n","    print(f\"[INFO] Training GNNs ({model_type}) with contrastive loss and early stopping on {device}...\")\n","    pathway_names = list(graphs.keys())\n","    cells_list = list(expr.index)\n","    pathway_embs = []\n","\n","    for pathway in pathway_names:\n","        G = graphs[pathway]\n","        x, genes, cells = node_features_bipartite_enhanced(G, expr, pathway_genes, pathway)\n","        all_nodes = genes + cells\n","        node_to_idx = {n: i for i, n in enumerate(all_nodes)}\n","        edges = [(node_to_idx[a], node_to_idx[b]) for a, b in G.edges() if a in node_to_idx and b in node_to_idx]\n","        if not edges: continue\n","\n","        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n","        x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n","\n","        model = UnifiedGNN(x.shape[1], hidden_dim, out_dim, gnn_type=model_type).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","        best_loss = float(\"inf\")\n","        patience = 0\n","\n","        for epoch in range(epochs):\n","            optimizer.zero_grad()\n","            emb = model(x_tensor, edge_index)\n","            cell_emb = emb[len(genes):]\n","            loss_val, loss = contrastive_loss_dropout(cell_emb, drop_prob=0.2, temperature=0.5)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if loss_val + early_stop_delta < best_loss:\n","                best_loss = loss_val\n","                patience = 0\n","            else:\n","                patience += 1\n","\n","            if patience > early_stop_patience:\n","                print(f\"[INFO] Early stopped at epoch {epoch+1} for pathway '{pathway}' (best loss: {best_loss:.4f})\")\n","                break\n","\n","            if (epoch+1) % 50 == 0 or epoch == 0:\n","                print(f\"[INFO] Pathway '{pathway}' epoch {epoch+1}, contrastive loss: {loss_val:.4f}\")\n","\n","        cell_emb = emb[len(genes):].detach().cpu().numpy()\n","        cell_emb_dict = {cell: cell_emb[i] for i, cell in enumerate(cells)}\n","        emb_reordered = np.array([cell_emb_dict.get(cell, np.zeros(out_dim)) for cell in cells_list])\n","        pathway_embs.append(emb_reordered)\n","\n","    pathway_embs = np.stack(pathway_embs, axis=1)  # (cells, pathways, dim)\n","    pathway_embs_tensor = torch.tensor(pathway_embs, dtype=torch.float32).to(device)\n","\n","    print(f\"[INFO] Finished training GNNs. Pathway embeddings shape = {pathway_embs_tensor.shape}\")\n","    return pathway_embs_tensor, pathway_names, cells_list\n","\n","\n","# ---- 7. Pathway Attention ----\n","class PathwayAttention(nn.Module):\n","    def __init__(self, pathway_dim, num_pathways, temperature=1.0, entropy_reg=0.01):\n","        super().__init__()\n","        self.attn_layer = nn.Linear(pathway_dim, 1, bias=False)\n","        self.temperature = temperature\n","        self.entropy_reg = entropy_reg\n","\n","    def forward(self, pathway_embs):\n","        attn_scores = self.attn_layer(pathway_embs)[:,:,0]  # (cells, pathways)\n","        attn_weights = F.softmax(attn_scores / self.temperature, dim=1)  # scaled softmax\n","        weighted_emb = torch.sum(pathway_embs * attn_weights.unsqueeze(-1), dim=1)  # (cells, dim)\n","        entropy = -torch.sum(attn_weights * torch.log(attn_weights + 1e-10), dim=1).mean()\n","        return weighted_emb, attn_weights, entropy * self.entropy_reg\n","\n","\n","# ---- 8. Helper: Run Attention with multiple temps ----\n","def run_attention(pathway_embs_tensor, pathway_names, cells,\n","                  temperatures=[0.33, 0.7, 0.9, 1.2, 1.5], entropy_reg=0.01, device=\"cuda\"):\n","    results = {}\n","    pathway_embs_tensor = pathway_embs_tensor.to(device)\n","\n","    for T in temperatures:\n","        print(f\"\\n[INFO] Running attention with temperature={T} on {device}...\")\n","        attn_model = PathwayAttention(\n","            pathway_dim=pathway_embs_tensor.shape[-1],\n","            num_pathways=pathway_embs_tensor.shape[1],\n","            temperature=T,\n","            entropy_reg=entropy_reg\n","        ).to(device)\n","        attn_model.eval()\n","        with torch.no_grad():\n","            final_emb_tensor, attn_weights_tensor, entropy_loss = attn_model(pathway_embs_tensor)\n","        final_emb = final_emb_tensor.cpu().numpy()\n","        attn_weights = attn_weights_tensor.cpu().numpy()\n","        results[T] = {\n","            \"final_emb\": final_emb,\n","            \"attn_weights\": attn_weights,\n","            \"entropy_loss\": entropy_loss.item()\n","        }\n","        print(f\"[INFO] Attention done (T={T}). Entropy={entropy_loss.item():.4f}\")\n","    return results\n","\n","# ---- 8. Leiden and KMeans Clustering, Evaluation ----\n","def cluster_and_eval_scanpy(final_emb, cells, adata, n_clusters=None, leiden_res_list=None):\n","    print(\"[INFO] Running clustering (Leiden + KMeans)...\")\n","\n","    # Default Leiden resolutions to test\n","    if leiden_res_list is None:\n","        leiden_res_list = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 2.0]\n","\n","    # Add embeddings to AnnData\n","    adata.obsm['X_gnn'] = final_emb\n","    sc.pp.neighbors(adata, use_rep='X_gnn', n_neighbors=15)\n","\n","    leiden_silhouettes = []\n","    leiden_label_sets = {}\n","\n","    #  Sweep Leiden resolutions\n","    for res in leiden_res_list:\n","        sc.tl.leiden(adata, resolution=res, key_added=f'gnn_leiden_{res}')\n","        leiden_labels = adata.obs[f'gnn_leiden_{res}'].astype(str).reindex(cells).fillna('NA').tolist()\n","        leiden_labels_num = pd.factorize(leiden_labels)[0]\n","\n","        if len(set(leiden_labels_num)) > 1:  # silhouette needs >1 cluster\n","            sil = silhouette_score(final_emb, leiden_labels_num)\n","        else:\n","            sil = -1  # invalid\n","        leiden_silhouettes.append(sil)\n","        leiden_label_sets[res] = leiden_labels\n","        print(f\"[INFO] Leiden res={res:.2f}, clusters={len(set(leiden_labels))}, silhouette={sil:.3f}\")\n","\n","    # Pick best Leiden resolution\n","    best_res = leiden_res_list[np.argmax(leiden_silhouettes)]\n","    best_leiden_labels = leiden_label_sets[best_res]\n","    best_leiden_silhouette = max(leiden_silhouettes)\n","    print(f\"[INFO] Best Leiden resolution={best_res}, silhouette={best_leiden_silhouette:.3f}\")\n","\n","    #  KMeans backup\n","    if n_clusters is None:\n","        sil_scores = []\n","        for k in range(3, 15):\n","            labels = KMeans(n_clusters=k).fit_predict(final_emb)\n","            sil_scores.append(silhouette_score(final_emb, labels))\n","        n_clusters = np.argmax(sil_scores) + 3\n","        print(f\"[INFO] Using elbow method: n_clusters={n_clusters}\")\n","\n","    elif n_clusters == \"unique\":\n","        n_clusters = adata.obs['assigned_cluster'].nunique()\n","        print(f\"[INFO] Using unique assigned clusters: n_clusters={n_clusters}\")\n","\n","    kmeans_labels = KMeans(n_clusters=n_clusters).fit_predict(final_emb)\n","    kmeans_silhouette = silhouette_score(final_emb, kmeans_labels)\n","    print(f\"[INFO] KMeans n={n_clusters}, silhouette={kmeans_silhouette:.3f}\")\n","\n","    #  Evaluation (if reference labels exist)\n","    if 'assigned_cluster' in adata.obs:\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster']\n","        metrics = {\n","            'Leiden ARI': adjusted_rand_score(true_labels, best_leiden_labels),\n","            'Leiden NMI': normalized_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden AMI': adjusted_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden Silhouette': best_leiden_silhouette,\n","            'KMeans ARI': adjusted_rand_score(true_labels, kmeans_labels),\n","            'KMeans NMI': normalized_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans AMI': adjusted_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans Silhouette': kmeans_silhouette\n","        }\n","        print(\"[INFO] Clustering metrics:\")\n","        for k, v in metrics.items():\n","            print(f\"  {k}: {v:.3f}\")\n","\n","        # Fix label alignment and types\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster'].astype(str).values\n","        pred_labels = kmeans_labels.astype(str)\n","\n","        # Crosstab for rectangular confusion matrix\n","        cm_df = pd.crosstab(pd.Series(true_labels, name=\"True\"), pd.Series(pred_labels, name=\"Predicted\"))\n","\n","        plt.figure(figsize=(12,8))\n","        sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n","        plt.xlabel(\"Predicted Clusters\")\n","        plt.ylabel(\"True Labels (Assigned Clusters)\")\n","        plt.title(\"Confusion Matrix : True vs Predicted\") #(Crosstab)\n","        plt.tight_layout()\n","        plt.show()\n","\n","    else:\n","        print(\"[INFO] No reference cluster labels found.\")\n","\n","    return best_leiden_labels, kmeans_labels, best_res, best_leiden_silhouette, kmeans_silhouette\n","\n","# ---- 9. Pathway Attribution per Cluster ----\n","def pathway_attribution(attn_weights, pathway_names, cluster_labels, cells, top_n=5):\n","    print(\"[INFO] Calculating pathway attribution per cluster...\")\n","    attention_df = pd.DataFrame(attn_weights, index=cells, columns=pathway_names)\n","    results = {}\n","    clusters = sorted(set(cluster_labels))\n","    for cluster in clusters:\n","        cluster_cells = [cell for cell, lbl in zip(cells, cluster_labels) if lbl == cluster]\n","        other_cells = [cell for cell, lbl in zip(cells, cluster_labels) if lbl != cluster]\n","        cluster_attention = attention_df.loc[cluster_cells].mean()\n","        pathway_pvals = {}\n","        for pathway in pathway_names:\n","            vals1 = attention_df.loc[cluster_cells, pathway]\n","            vals2 = attention_df.loc[other_cells, pathway]\n","            try:\n","                _, pval = ttest_ind(vals1, vals2, equal_var=False)\n","            except:\n","                pval = 1.0\n","            pathway_pvals[pathway] = pval\n","        sorted_pathways = cluster_attention.sort_values(ascending=False)\n","        results[cluster] = []\n","        for pathway in sorted_pathways.head(top_n).index:\n","            importance = cluster_attention[pathway]\n","            pval = pathway_pvals[pathway]\n","            significance = (\n","                \"***\" if pval < 0.001 else\n","                \"**\" if pval < 0.01 else\n","                \"*\" if pval < 0.05 else \"\"\n","            )\n","            results[cluster].append({\n","                \"pathway\": pathway,\n","                \"importance\": importance,\n","                \"pval\": pval,\n","                \"significance\": significance\n","            })\n","    print(\"[INFO] Pathway attribution completed.\")\n","    return results\n","\n","def print_cluster_pathways(cluster_results, cluster_labels, cells):\n","    for cluster, pathways in cluster_results.items():\n","        cluster_cells = [cell for cell, lbl in zip(cells, cluster_labels) if lbl == cluster]\n","        print(f\"\\nCluster {cluster} ({len(cluster_cells)} cells):\")\n","        for pw in pathways:\n","            print(f\"  {pw['pathway']}: {pw['importance']:.3f} (p={pw['pval']:.3f}) {pw['significance']}\")\n","\n","# ---- 10. Visualizations ----\n","def visualize(final_emb, cluster_labels, attn_weights, pathway_names, cells, leiden_labels=None):\n","    print(\"[INFO] Generating visualizations (PCA, UMAP, heatmap)...\")\n","    # PCA\n","    pca = PCA(n_components=2)\n","    pca_proj = pca.fit_transform(final_emb)\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=pca_proj[:,0], y=pca_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(f'PCA of Cell Embeddings (Var explained: {pca.explained_variance_ratio_.sum():.2f})')\n","    plt.xlabel(\"PCA1\")\n","    plt.ylabel(\"PCA2\")\n","    plt.legend(title=\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","    # UMAP\n","    reducer = umap.UMAP(n_components=2, random_state=42)\n","    umap_proj = reducer.fit_transform(final_emb)\n","    plt.figure(figsize=(6,5))\n","    if leiden_labels is not None:\n","        sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=leiden_labels, palette='tab20', s=15)\n","        plt.title(\"UMAP of Cell Embeddings (Leiden)\")\n","        plt.legend(title=\"Leiden\")\n","    else:\n","        sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","        plt.title(\"UMAP of Cell Embeddings\")\n","        plt.legend(title=\"Cluster\")\n","    plt.xlabel(\"UMAP1\")\n","    plt.ylabel(\"UMAP2\")\n","    plt.tight_layout()\n","    plt.show()\n","    #kmeans umap\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(\"UMAP of Cell Embeddings\")\n","    plt.legend(title=\"Cluster\")\n","    plt.xlabel(\"UMAP1\")\n","    plt.ylabel(\"UMAP2\")\n","    plt.tight_layout()\n","    plt.show()\n","    # Pathway-cluster heatmap\n","    attention_df = pd.DataFrame(attn_weights, index=cells, columns=pathway_names)\n","    cluster_attention_matrix = []\n","    clusters = sorted(set(cluster_labels))\n","    for cluster in clusters:\n","        cluster_cells = [cell for cell, lbl in zip(cells, cluster_labels) if lbl == cluster]\n","        cluster_attention_matrix.append(attention_df.loc[cluster_cells].mean().values)\n","    cluster_attention_matrix = np.array(cluster_attention_matrix)\n","    top_k = min(20, len(pathway_names))\n","    avg_attention = np.mean(cluster_attention_matrix, axis=0)\n","    top_k_indices = np.argsort(avg_attention)[-top_k:]\n","    heatmap_matrix = cluster_attention_matrix[:, top_k_indices]\n","    heatmap_labels = [pathway_names[i][:30] for i in top_k_indices]\n","    plt.figure(figsize=(10,6))\n","    sns.heatmap(heatmap_matrix, xticklabels=heatmap_labels, yticklabels=[f\"Cluster {i}\" for i in clusters], cmap='viridis')\n","    plt.title(\"Top Pathway Attention per Cluster\")\n","    plt.xlabel(\"Pathway\")\n","    plt.ylabel(\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","    print(\"[INFO] Visualizations completed.\")\n","\n","# ---- 9. Pipeline ----\n","def run_pipeline(\n","    expr_csv, pathway_file, metadata_cols=['barcode', 'assigned_cluster'],\n","    regress_vars=None, hvg_n=2000, min_genes=10, max_genes=300, k=5,\n","    model_type='sage', hidden_dim=32, out_dim=16, epochs=100,\n","    early_stop_patience=15, early_stop_delta=1e-4,\n","    top_n_pathways=5, top_n_marker_genes=5, n_clusters=None, rare_cell_thresh=50,\n","    temperatures=[0.7], entropy_reg=0.01, device=\"cuda\"\n","):\n","    print(\"[INFO] Loading expression matrix...\")\n","    df = pd.read_csv(expr_csv, index_col=0)\n","    metadata = df[metadata_cols]\n","    expr_df = df.drop(columns=metadata_cols)\n","    expr_df = expr_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n","    adata = sc.AnnData(expr_df)\n","    adata.obs = metadata\n","\n","    # Fix the duplicate issue\n","    print(f\"[INFO] Before fixing: {adata.obs_names.duplicated().sum()} duplicate cell names\")\n","    print(f\"[INFO] Before fixing: {adata.var_names.duplicated().sum()} duplicate gene names\")\n","\n","    # Make observation names (cells) unique\n","    if adata.obs_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate cell names. Making unique...\")\n","        adata.obs_names_make_unique()\n","\n","    # Make variable names (genes) unique\n","    if adata.var_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate gene names. Making unique...\")\n","        adata.var_names_make_unique()\n","\n","    print(f\"[INFO] After fixing: {adata.obs_names.duplicated().sum()} duplicate cell names\")\n","    print(f\"[INFO] After fixing: {adata.var_names.duplicated().sum()} duplicate gene names\")\n","\n","    # Remove rare cell clusters\n","    if 'assigned_cluster' in adata.obs:\n","        counts = adata.obs['assigned_cluster'].value_counts()\n","        keep = counts[counts >= rare_cell_thresh].index\n","        before = adata.n_obs\n","        adata = adata[adata.obs['assigned_cluster'].isin(keep)]\n","        print(f\"[INFO] Removed rare cell types (<{rare_cell_thresh} cells). {before} → {adata.n_obs} cells kept.\")\n","\n","    adata = preprocess_sctransform(adata, regress_vars=regress_vars, hvg_n=hvg_n)\n","\n","    # Pathway parsing\n","    print(\"[INFO] Parsing pathway file...\")\n","    pathway_genes = {}\n","    genes_in_data = set(adata.var_names)\n","    with open(pathway_file, 'r') as f:\n","        for line in f:\n","            parts = line.strip().split('\\t')\n","            pathway, genes = parts[0], parts[2:]\n","            filtered = [g for g in genes if g in genes_in_data]\n","            if min_genes <= len(filtered) <= max_genes:\n","                pathway_genes[pathway] = filtered\n","    print(f\"[INFO] Pathway parsing completed. Filtered pathways: {len(pathway_genes)}\")\n","\n","    # Graph construction\n","    graphs = construct_pathway_bipartite_graphs(adata, pathway_genes, k=k)\n","    expr = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n","\n","    # ---- Train GNNs once ----\n","    pathway_embs_tensor, pathway_names, cells = train_pathway_gnns(\n","        graphs, expr, pathway_genes, model_type=model_type,\n","        hidden_dim=hidden_dim, out_dim=out_dim, epochs=epochs,\n","        early_stop_patience=early_stop_patience, early_stop_delta=early_stop_delta\n","    )\n","\n","    # ---- Run Attention with multiple temperatures ----\n","    attn_results = run_attention(pathway_embs_tensor, pathway_names, cells, temperatures, entropy_reg)\n","\n","    # ---- Cluster + Evaluate for each temp ----\n","    for T, res in attn_results.items():\n","        print(f\"\\n====== Results for Temperature={T} ======\")\n","        leiden_labels, kmeans_labels, best_res, leiden_sil, kmeans_sil = cluster_and_eval_scanpy(\n","            res[\"final_emb\"], cells, adata, n_clusters=n_clusters\n","        )\n","        cluster_results = pathway_attribution(res[\"attn_weights\"], pathway_names, kmeans_labels, cells, top_n=top_n_pathways)\n","        print_cluster_pathways(cluster_results, kmeans_labels, cells)\n","\n","        # ---- Marker Gene Enrichment using Scanpy ----\n","        print(\"[INFO] Running marker gene analysis with Scanpy (Wilcoxon test)...\")\n","        adata.obs[\"cluster\"] = kmeans_labels.astype(str)\n","        sc.tl.rank_genes_groups(adata, groupby=\"cluster\", method=\"wilcoxon\", use_raw=False)\n","        sc.pl.rank_genes_groups(adata, n_genes=top_n_marker_genes, sharey=False)\n","        markers_df = sc.get.rank_genes_groups_df(adata, group=None)\n","        for c in sorted(adata.obs[\"cluster\"].unique()):\n","            top_genes = markers_df[markers_df[\"group\"] == c].head(top_n_marker_genes)\n","            print(f\"\\nCluster {c} top marker genes:\")\n","            for _, row in top_genes.iterrows():\n","                sig = (\n","                    \"***\" if row[\"pvals_adj\"] < 0.001 else\n","                    \"**\" if row[\"pvals_adj\"] < 0.01 else\n","                    \"*\" if row[\"pvals_adj\"] < 0.05 else \"\"\n","                )\n","                print(f\"  {row['names']}: logFC={row['logfoldchanges']:.2f}, adj.p={row['pvals_adj']:.3g} {sig}\")\n","\n","        visualize(res[\"final_emb\"], kmeans_labels, res[\"attn_weights\"], pathway_names, cells, leiden_labels)\n","\n","# Example usage:\n","run_pipeline(\n","   'your_scRNA seq_csv_dataset',\n","   'your_pathway_dataset',\n","   model_type='gcn', epochs=150, early_stop_patience=15, n_clusters=\"unique\"\n"," )"]},{"cell_type":"markdown","source":["**Simple Baseline Bipartite**\n","\n"],"metadata":{"id":"vQ-ObRAkHUkS"}},{"cell_type":"code","source":["# ---- DEVICE SETUP ----\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"[INFO] Using device:\", device)\n","\n","# ---- 1. Same preprocessing as main method ----\n","def preprocess_sctransform(adata, regress_vars=None, hvg_n=2000):\n","    print(\"[INFO] Preprocessing with SCTransform-like workflow (highly variable genes + regression)...\")\n","    sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=hvg_n)\n","    adata = adata[:, adata.var['highly_variable']]\n","    if regress_vars is not None:\n","        sc.pp.regress_out(adata, regress_vars)\n","    sc.pp.scale(adata)\n","    print(\"[INFO] Preprocessing completed.\")\n","    return adata\n","\n","# ---- 2. Simple Bipartite Graph Construction (NO pathway grouping) ----\n","def construct_simple_bipartite_graph(\n","    adata,\n","    k=5,\n","    min_expression=0.25,\n","    gene_percentile=75,\n","    min_edges_per_gene=2,\n","    fallback_topk=True\n","):\n","    \"\"\"\n","    Build a SINGLE bipartite graph (genes ↔ cells) using ALL genes together\n","    WITHOUT pathway-specific grouping. This is the fair baseline.\n","\n","    Uses the same edge construction logic as pathway-aware method:\n","    - Selective edges based on expression thresholds\n","    - Same fallback mechanism\n","    BUT treats all genes as one big \"pathway\" to isolate the effect of pathway information\n","    \"\"\"\n","    print(\"[INFO] Building simple bipartite graph (all genes, no pathway grouping)...\")\n","    expr = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n","\n","    genes = list(adata.var_names)\n","    cells = list(adata.obs_names)\n","\n","    G = nx.Graph()\n","    G.add_nodes_from(genes, bipartite=0, node_type='gene')\n","    G.add_nodes_from(cells, bipartite=1, node_type='cell')\n","\n","    # ---- selective edges by threshold (same logic as pathway method) ----\n","    edges_added = 0\n","    for gene in genes:\n","        gene_expr = expr[gene]\n","\n","        # robust per-gene threshold on non-zero values\n","        if (gene_expr > 0).sum() > 0:\n","            per_gene_thr = np.percentile(gene_expr[gene_expr > 0], gene_percentile)\n","        else:\n","            per_gene_thr = float('inf')\n","\n","        thr = max(per_gene_thr, min_expression)\n","\n","        # add edges for cells where expression is \"meaningfully high\"\n","        high_cells = gene_expr.index[gene_expr > thr]\n","        for cell in high_cells:\n","            val = gene_expr.loc[cell]\n","            val_scalar = val.iloc[0] if hasattr(val, 'iloc') else val\n","            if val_scalar > 0:\n","                G.add_edge(gene, cell, weight=float(val_scalar))\n","                edges_added += 1\n","\n","    # ---- density check + fallback ----\n","    needed_edges = len(genes) * min_edges_per_gene\n","    if edges_added < needed_edges and fallback_topk:\n","        print(f\"[INFO] Graph sparse ({edges_added} edges), applying top-k fallback...\")\n","        expr_array = expr.to_numpy()\n","\n","        # normalize per cell\n","        row_max = expr_array.max(axis=1, keepdims=True)\n","        row_max[row_max == 0] = 1.0\n","        norm_expr = expr_array / row_max\n","\n","        for i, cell in enumerate(cells):\n","            cell_vec = norm_expr[i]\n","            k_eff = min(k, (cell_vec > 0).sum()) if (cell_vec > 0).any() else 0\n","            if k_eff > 0:\n","                top_idx = np.argpartition(cell_vec, -k_eff)[-k_eff:]\n","                for gi in top_idx:\n","                    gene = genes[gi]\n","                    if not G.has_edge(gene, cell):\n","                        raw_val = expr_array[i, gi]\n","                        if raw_val > 0:\n","                            G.add_edge(gene, cell, weight=float(raw_val))\n","                            edges_added += 1\n","\n","    print(f\"[INFO] Bipartite graph constructed: {len(genes)} genes, {len(cells)} cells, {edges_added} edges\")\n","    return G, genes, cells\n","\n","# ---- 3. Node Features for Bipartite Graph (same as pathway method) ----\n","def node_features_bipartite_simple(G, expr, n_pca=5):\n","    \"\"\"\n","    Create node features for bipartite graph - same logic as pathway method\n","    but without pathway-specific information\n","    \"\"\"\n","    genes = [n for n, d in G.nodes(data=True) if d['node_type'] == 'gene']\n","    cells = [n for n, d in G.nodes(data=True) if d['node_type'] == 'cell']\n","\n","    # PCA for cells\n","    if len(genes) > 1 and len(cells) > 1:\n","        pca = PCA(n_components=min(n_pca, len(genes)))\n","        cell_pca = pca.fit_transform(expr.loc[cells, genes])\n","        pca_dim = cell_pca.shape[1]\n","    else:\n","        cell_pca = np.zeros((len(cells), n_pca))\n","        pca_dim = n_pca\n","\n","    features = []\n","    # ---- Genes ----\n","    for gene in genes:\n","        vals = expr[gene] if gene in expr.columns else np.zeros(len(cells))\n","        # Base features: [mean, std, degree, type_gene, type_cell]\n","        vec = [np.mean(vals), np.std(vals), G.degree(gene), 1, 0]\n","        # Pad PCA slots (genes don't get PCA features)\n","        vec += [0.0] * pca_dim\n","        features.append(vec)\n","\n","    # ---- Cells ----\n","    for i, cell in enumerate(cells):\n","        vals = expr.loc[cell, genes] if cell in expr.index else np.zeros(len(genes))\n","        vec = [np.mean(vals), np.std(vals), G.degree(cell), 0, 1]\n","        # Add PCA profile\n","        vec += list(cell_pca[i])\n","        features.append(vec)\n","\n","    x = np.array(features, dtype=np.float32)\n","    return x, genes, cells\n","\n","# ---- 4. Simple GNN Model ----\n","class SimpleGNN(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, out_dim, gnn_type='gcn', heads=2):\n","        super().__init__()\n","        self.gnn_type = gnn_type.lower()\n","        if self.gnn_type == 'gcn':\n","            self.conv1 = GCNConv(in_dim, hidden_dim)\n","            self.conv2 = GCNConv(hidden_dim, out_dim)\n","        elif self.gnn_type == 'sage':\n","            self.conv1 = SAGEConv(in_dim, hidden_dim)\n","            self.conv2 = SAGEConv(hidden_dim, out_dim)\n","        elif self.gnn_type == 'gat':\n","            self.conv1 = GATConv(in_dim, hidden_dim, heads=heads, concat=False)\n","            self.conv2 = GATConv(hidden_dim, out_dim, heads=heads, concat=False)\n","        else:\n","            raise ValueError(\"gnn_type must be one of: 'gcn', 'sage', 'gat'\")\n","\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x, edge_index):\n","        x = F.relu(self.conv1(x, edge_index))\n","        x = self.dropout(x)\n","        x = self.conv2(x, edge_index)\n","        return x\n","\n","# ---- 5. Contrastive Loss (same as main method) ----\n","def contrastive_loss_dropout(cell_emb, drop_prob=0.2, temperature=0.5):\n","    mask = (torch.rand_like(cell_emb) > drop_prob).float()\n","    cell_emb_aug = cell_emb * mask\n","    N = cell_emb.shape[0]\n","    z1 = F.normalize(cell_emb, dim=1)\n","    z2 = F.normalize(cell_emb_aug, dim=1)\n","    representations = torch.cat([z1, z2], dim=0)\n","    similarity_matrix = torch.matmul(representations, representations.T)\n","    mask_eye = torch.eye(2*N, dtype=torch.bool, device=cell_emb.device)\n","    similarity_matrix = similarity_matrix.masked_fill(mask_eye, -9e15)\n","    positives = torch.cat([torch.diag(similarity_matrix, N), torch.diag(similarity_matrix, -N)], dim=0)\n","    negatives = similarity_matrix[~mask_eye].view(2*N, -1)\n","    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1) / temperature\n","    labels = torch.zeros(2*N, dtype=torch.long, device=cell_emb.device)\n","    loss = F.cross_entropy(logits, labels)\n","    return loss.item(), loss\n","\n","# ---- 6. Training (modified for bipartite graph) ----\n","def train_simple_bipartite_gnn(G, expr, model_type='sage', hidden_dim=32, out_dim=16,\n","                               epochs=100, early_stop_patience=15, early_stop_delta=1e-4, device=\"cuda\"):\n","    print(f\"[INFO] Training simple bipartite GNN ({model_type}) with contrastive loss...\")\n","\n","    # Get node features\n","    x, genes, cells = node_features_bipartite_simple(G, expr, n_pca=5)\n","\n","    all_nodes = genes + cells\n","    node_to_idx = {n: i for i, n in enumerate(all_nodes)}\n","\n","    # Convert graph to PyTorch format\n","    edges = [(node_to_idx[a], node_to_idx[b]) for a, b in G.edges() if a in node_to_idx and b in node_to_idx]\n","    if not edges:\n","        print(\"[ERROR] No edges in graph!\")\n","        return None, cells\n","\n","    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n","    x_tensor = torch.tensor(x, dtype=torch.float32).to(device)\n","\n","    # Model setup\n","    model = SimpleGNN(x.shape[1], hidden_dim, out_dim, gnn_type=model_type).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n","\n","    best_loss = float(\"inf\")\n","    patience = 0\n","\n","    for epoch in range(epochs):\n","        optimizer.zero_grad()\n","        emb = model(x_tensor, edge_index)\n","\n","        # Extract cell embeddings (same as pathway method)\n","        cell_emb = emb[len(genes):]\n","\n","        loss_val, loss = contrastive_loss_dropout(cell_emb, drop_prob=0.2, temperature=0.5)\n","        loss.backward()\n","        optimizer.step()\n","\n","        if loss_val + early_stop_delta < best_loss:\n","            best_loss = loss_val\n","            patience = 0\n","        else:\n","            patience += 1\n","\n","        if patience > early_stop_patience:\n","            print(f\"[INFO] Early stopped at epoch {epoch+1} (best loss: {best_loss:.4f})\")\n","            break\n","\n","        if (epoch+1) % 50 == 0 or epoch == 0:\n","            print(f\"[INFO] Epoch {epoch+1}, contrastive loss: {loss_val:.4f}\")\n","\n","    # Get final cell embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        final_emb = model(x_tensor, edge_index)\n","        cell_emb = final_emb[len(genes):].cpu().numpy()\n","\n","    print(f\"[INFO] Training completed. Cell embeddings shape: {cell_emb.shape}\")\n","    return cell_emb, cells\n","\n","# ---- 7. Clustering and Evaluation (same as main method) ----\n","def cluster_and_eval_simple(final_emb, cells, adata, n_clusters=None, leiden_res_list=None):\n","    print(\"[INFO] Running clustering (Leiden + KMeans)...\")\n","\n","    if leiden_res_list is None:\n","        leiden_res_list = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 2.0]\n","\n","    adata.obsm['X_simple_gnn'] = final_emb\n","    sc.pp.neighbors(adata, use_rep='X_simple_gnn', n_neighbors=15)\n","\n","    leiden_silhouettes = []\n","    leiden_label_sets = {}\n","\n","    for res in leiden_res_list:\n","        sc.tl.leiden(adata, resolution=res, key_added=f'simple_leiden_{res}')\n","        leiden_labels = adata.obs[f'simple_leiden_{res}'].astype(str).reindex(cells).fillna('NA').tolist()\n","        leiden_labels_num = pd.factorize(leiden_labels)[0]\n","\n","        if len(set(leiden_labels_num)) > 1:\n","            sil = silhouette_score(final_emb, leiden_labels_num)\n","        else:\n","            sil = -1\n","        leiden_silhouettes.append(sil)\n","        leiden_label_sets[res] = leiden_labels\n","        print(f\"[INFO] Leiden res={res:.2f}, clusters={len(set(leiden_labels))}, silhouette={sil:.3f}\")\n","\n","    best_res = leiden_res_list[np.argmax(leiden_silhouettes)]\n","    best_leiden_labels = leiden_label_sets[best_res]\n","    best_leiden_silhouette = max(leiden_silhouettes)\n","\n","    # KMeans\n","    if n_clusters is None:\n","        sil_scores = []\n","        for k in range(3, 15):\n","            labels = KMeans(n_clusters=k).fit_predict(final_emb)\n","            sil_scores.append(silhouette_score(final_emb, labels))\n","        n_clusters = np.argmax(sil_scores) + 3\n","    elif n_clusters == \"unique\":\n","        n_clusters = adata.obs['assigned_cluster'].nunique()\n","\n","    kmeans_labels = KMeans(n_clusters=n_clusters).fit_predict(final_emb)\n","    kmeans_silhouette = silhouette_score(final_emb, kmeans_labels)\n","\n","    # Evaluation\n","    if 'assigned_cluster' in adata.obs:\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster']\n","        metrics = {\n","            'Leiden ARI': adjusted_rand_score(true_labels, best_leiden_labels),\n","            'Leiden NMI': normalized_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden AMI': adjusted_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden Silhouette': best_leiden_silhouette,\n","            'KMeans ARI': adjusted_rand_score(true_labels, kmeans_labels),\n","            'KMeans NMI': normalized_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans AMI': adjusted_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans Silhouette': kmeans_silhouette\n","        }\n","        print(\"[INFO] Simple GNN Clustering metrics:\")\n","        for k, v in metrics.items():\n","            print(f\"  {k}: {v:.3f}\")\n","\n","        # Confusion matrix\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster'].astype(str).values\n","        pred_labels = kmeans_labels.astype(str)\n","        cm_df = pd.crosstab(pd.Series(true_labels, name=\"True\"), pd.Series(pred_labels, name=\"Predicted\"))\n","        plt.figure(figsize=(12,8))\n","        sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n","        plt.xlabel(\"Predicted Clusters\")\n","        plt.ylabel(\"True Labels\")\n","        plt.title(\"Simple GNN: Confusion Matrix\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return best_leiden_labels, kmeans_labels, best_res, best_leiden_silhouette, kmeans_silhouette\n","\n","# ---- 8. Visualization ----\n","def visualize_simple(final_emb, cluster_labels, cells, leiden_labels=None):\n","    print(\"[INFO] Generating visualizations (PCA, UMAP)...\")\n","\n","    # PCA\n","    pca = PCA(n_components=2)\n","    pca_proj = pca.fit_transform(final_emb)\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=pca_proj[:,0], y=pca_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(f'Simple GNN: PCA (Var explained: {pca.explained_variance_ratio_.sum():.2f})')\n","    plt.xlabel(\"PCA1\")\n","    plt.ylabel(\"PCA2\")\n","    plt.legend(title=\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # UMAP\n","    reducer = umap.UMAP(n_components=2, random_state=42)\n","    umap_proj = reducer.fit_transform(final_emb)\n","\n","    if leiden_labels is not None:\n","        plt.figure(figsize=(6,5))\n","        sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=leiden_labels, palette='tab20', s=15)\n","        plt.title(\"Simple GNN: UMAP (Leiden)\")\n","        plt.xlabel(\"UMAP1\")\n","        plt.ylabel(\"UMAP2\")\n","        plt.legend(title=\"Leiden\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(\"Simple GNN: UMAP (KMeans)\")\n","    plt.xlabel(\"UMAP1\")\n","    plt.ylabel(\"UMAP2\")\n","    plt.legend(title=\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","# ---- 9. Main Pipeline (updated for bipartite graph) ----\n","def run_simple_baseline_pipeline(\n","    expr_csv, metadata_cols=['barcode', 'assigned_cluster'],\n","    regress_vars=None, hvg_n=2000,\n","    k=5, min_expression=0.25, gene_percentile=75,  # same params as pathway method\n","    model_type='sage', hidden_dim=32, out_dim=16, epochs=100,\n","    early_stop_patience=15, early_stop_delta=1e-4,\n","    top_n_marker_genes=5, n_clusters=None, rare_cell_thresh=40,\n","    device=\"cuda\"\n","):\n","    print(\"[INFO] === SIMPLE BIPARTITE GNN BASELINE (No Pathway Information) ===\")\n","    print(\"[INFO] Loading expression matrix...\")\n","\n","    df = pd.read_csv(expr_csv, index_col=0)\n","    metadata = df[metadata_cols]\n","    expr_df = df.drop(columns=metadata_cols)\n","    expr_df = expr_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n","\n","    adata = sc.AnnData(expr_df)\n","    adata.obs = metadata\n","\n","    # Fix duplicate names\n","    if adata.obs_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate cell names. Making unique...\")\n","        adata.obs_names_make_unique()\n","    if adata.var_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate gene names. Making unique...\")\n","        adata.var_names_make_unique()\n","\n","    # Remove rare cell clusters\n","    if 'assigned_cluster' in adata.obs:\n","        counts = adata.obs['assigned_cluster'].value_counts()\n","        keep = counts[counts >= rare_cell_thresh].index\n","        before = adata.n_obs\n","        adata = adata[adata.obs['assigned_cluster'].isin(keep)]\n","        print(f\"[INFO] Removed rare cell types (<{rare_cell_thresh} cells). {before} → {adata.n_obs} cells kept.\")\n","\n","    # Preprocess (same as pathway method)\n","    adata = preprocess_sctransform(adata, regress_vars=regress_vars, hvg_n=hvg_n)\n","\n","    # Build simple bipartite graph (all genes together, no pathway grouping)\n","    G, genes, cells = construct_simple_bipartite_graph(\n","        adata, k=k, min_expression=min_expression,\n","        gene_percentile=gene_percentile, fallback_topk=True\n","    )\n","\n","    expr = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n","\n","    # Train GNN\n","    final_emb, cells_ordered = train_simple_bipartite_gnn(\n","        G, expr, model_type=model_type, hidden_dim=hidden_dim, out_dim=out_dim,\n","        epochs=epochs, early_stop_patience=early_stop_patience, early_stop_delta=early_stop_delta,\n","        device=device\n","    )\n","\n","    if final_emb is None:\n","        print(\"[ERROR] Training failed!\")\n","        return\n","\n","    # Clustering and evaluation\n","    leiden_labels, kmeans_labels, best_res, leiden_sil, kmeans_sil = cluster_and_eval_simple(\n","        final_emb, cells_ordered, adata, n_clusters=n_clusters\n","    )\n","\n","    # Marker gene analysis\n","    print(\"[INFO] Running marker gene analysis...\")\n","    adata.obs[\"cluster\"] = kmeans_labels.astype(str)\n","    sc.tl.rank_genes_groups(adata, groupby=\"cluster\", method=\"wilcoxon\", use_raw=False)\n","    sc.pl.rank_genes_groups(adata, n_genes=top_n_marker_genes, sharey=False)\n","\n","    markers_df = sc.get.rank_genes_groups_df(adata, group=None)\n","    for c in sorted(adata.obs[\"cluster\"].unique()):\n","        top_genes = markers_df[markers_df[\"group\"] == c].head(top_n_marker_genes)\n","        print(f\"\\nCluster {c} top marker genes:\")\n","        for _, row in top_genes.iterrows():\n","            sig = (\n","                \"***\" if row[\"pvals_adj\"] < 0.001 else\n","                \"**\" if row[\"pvals_adj\"] < 0.01 else\n","                \"*\" if row[\"pvals_adj\"] < 0.05 else \"\"\n","            )\n","            print(f\"  {row['names']}: logFC={row['logfoldchanges']:.2f}, adj.p={row['pvals_adj']:.3g} {sig}\")\n","\n","    # Visualizations\n","    visualize_simple(final_emb, kmeans_labels, cells_ordered, leiden_labels)\n","\n","    print(\"[INFO] Simple bipartite GNN baseline completed!\")\n","\n","\n","# Example usage:\n","run_simple_baseline_pipeline(\n","    'your_scRNA seq_csv_dataset',\n","    model_type='gcn', epochs=150, n_clusters=\"unique\"\n",")"],"metadata":{"id":"HUv2Axp19nNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVUsYbbG3cmK"},"source":["**Simple Baseline MLP**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJIQhaEc3fsh"},"outputs":[],"source":["# ---- DEVICE SETUP ----\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"[INFO] Using device:\", device)\n","\n","# ---- 1. Same preprocessing as main method ----\n","def preprocess_sctransform(adata, regress_vars=None, hvg_n=2000):\n","    print(\"[INFO] Preprocessing with SCTransform-like workflow (highly variable genes + regression)...\")\n","    sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=hvg_n)\n","    adata = adata[:, adata.var['highly_variable']]\n","    if regress_vars is not None:\n","        sc.pp.regress_out(adata, regress_vars)\n","    sc.pp.scale(adata)\n","    print(\"[INFO] Preprocessing completed.\")\n","    return adata\n","\n","# ---- 2. MLP Autoencoder Model ----\n","class MLPAutoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dims=[512, 256, 128], latent_dim=32, dropout=0.2):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.latent_dim = latent_dim\n","\n","        # Encoder\n","        encoder_layers = []\n","        prev_dim = input_dim\n","        for hidden_dim in hidden_dims:\n","            encoder_layers.extend([\n","                nn.Linear(prev_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Dropout(dropout),\n","                nn.BatchNorm1d(hidden_dim)\n","            ])\n","            prev_dim = hidden_dim\n","\n","        # Latent layer\n","        encoder_layers.append(nn.Linear(prev_dim, latent_dim))\n","        self.encoder = nn.Sequential(*encoder_layers)\n","\n","        # Decoder\n","        decoder_layers = []\n","        prev_dim = latent_dim\n","        for hidden_dim in reversed(hidden_dims):\n","            decoder_layers.extend([\n","                nn.Linear(prev_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Dropout(dropout),\n","                nn.BatchNorm1d(hidden_dim)\n","            ])\n","            prev_dim = hidden_dim\n","\n","        # Output layer\n","        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n","        self.decoder = nn.Sequential(*decoder_layers)\n","\n","    def forward(self, x):\n","        latent = self.encoder(x)\n","        reconstructed = self.decoder(latent)\n","        return latent, reconstructed\n","\n","    def encode(self, x):\n","        return self.encoder(x)\n","\n","# ---- 3. Alternative: Simple MLP for dimensionality reduction ----\n","class SimpleMLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dims=[512, 256], output_dim=32, dropout=0.2):\n","        super().__init__()\n","        layers = []\n","        prev_dim = input_dim\n","\n","        for hidden_dim in hidden_dims:\n","            layers.extend([\n","                nn.Linear(prev_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Dropout(dropout),\n","                nn.BatchNorm1d(hidden_dim)\n","            ])\n","            prev_dim = hidden_dim\n","\n","        layers.append(nn.Linear(prev_dim, output_dim))\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)\n","\n","# ---- 4. Contrastive Loss for MLP ----\n","def contrastive_loss_mlp(embeddings, drop_prob=0.2, temperature=0.5):\n","    \"\"\"\n","    Contrastive loss for MLP embeddings using dropout augmentation\n","    \"\"\"\n","    # Dropout-based augmentation\n","    mask = (torch.rand_like(embeddings) > drop_prob).float()\n","    embeddings_aug = embeddings * mask\n","    N = embeddings.shape[0]\n","    z1 = F.normalize(embeddings, dim=1)\n","    z2 = F.normalize(embeddings_aug, dim=1)\n","    representations = torch.cat([z1, z2], dim=0)\n","    similarity_matrix = torch.matmul(representations, representations.T)\n","    mask_eye = torch.eye(2*N, dtype=torch.bool, device=embeddings.device)\n","    similarity_matrix = similarity_matrix.masked_fill(mask_eye, -9e15)\n","    positives = torch.cat([torch.diag(similarity_matrix, N), torch.diag(similarity_matrix, -N)], dim=0)\n","    negatives = similarity_matrix[~mask_eye].view(2*N, -1)\n","    logits = torch.cat([positives.unsqueeze(1), negatives], dim=1) / temperature\n","    labels = torch.zeros(2*N, dtype=torch.long, device=embeddings.device)\n","    loss = F.cross_entropy(logits, labels)\n","    return loss.item(), loss\n","\n","# ---- 5. Training Functions ----\n","def train_mlp_autoencoder(data, model_type='autoencoder', hidden_dims=[512, 256, 128],\n","                         latent_dim=32, epochs=100, batch_size=256, learning_rate=0.001,\n","                         early_stop_patience=15, early_stop_delta=1e-4, device=\"cuda\"):\n","    \"\"\"\n","    Train MLP autoencoder or simple MLP with contrastive loss\n","    \"\"\"\n","    print(f\"[INFO] Training MLP {model_type} with contrastive loss...\")\n","\n","    n_cells, n_genes = data.shape\n","    data_tensor = torch.tensor(data, dtype=torch.float32)\n","\n","    # Create DataLoader\n","    dataset = torch.utils.data.TensorDataset(data_tensor)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","    # Initialize model\n","    if model_type == 'autoencoder':\n","        model = MLPAutoencoder(n_genes, hidden_dims, latent_dim).to(device)\n","    else:  # simple MLP\n","        model = SimpleMLP(n_genes, hidden_dims, latent_dim).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    best_loss = float(\"inf\")\n","    patience = 0\n","\n","    for epoch in range(epochs):\n","        epoch_loss = 0\n","        n_batches = 0\n","\n","        for batch_data, in dataloader:\n","            batch_data = batch_data.to(device)\n","            optimizer.zero_grad()\n","\n","            if model_type == 'autoencoder':\n","                latent, reconstructed = model(batch_data)\n","                # Reconstruction loss\n","                recon_loss = F.mse_loss(reconstructed, batch_data)\n","                # Contrastive loss on latent space\n","                cont_loss_val, cont_loss = contrastive_loss_mlp(latent, drop_prob=0.2, temperature=0.5)\n","                # Combined loss\n","                total_loss = recon_loss + 0.1 * cont_loss  # weight contrastive loss lower\n","            else:\n","                embeddings = model(batch_data)\n","                cont_loss_val, total_loss = contrastive_loss_mlp(embeddings, drop_prob=0.2, temperature=0.5)\n","\n","            total_loss.backward()\n","            optimizer.step()\n","\n","            epoch_loss += total_loss.item()\n","            n_batches += 1\n","\n","        avg_loss = epoch_loss / n_batches\n","\n","        # Early stopping\n","        if avg_loss + early_stop_delta < best_loss:\n","            best_loss = avg_loss\n","            patience = 0\n","        else:\n","            patience += 1\n","\n","        if patience > early_stop_patience:\n","            print(f\"[INFO] Early stopped at epoch {epoch+1} (best loss: {best_loss:.4f})\")\n","            break\n","\n","        if (epoch+1) % 20 == 0 or epoch == 0:\n","            print(f\"[INFO] Epoch {epoch+1}, average loss: {avg_loss:.4f}\")\n","\n","    # Get final embeddings\n","    model.eval()\n","    with torch.no_grad():\n","        data_tensor = data_tensor.to(device)\n","        if model_type == 'autoencoder':\n","            final_emb, _ = model(data_tensor)\n","        else:\n","            final_emb = model(data_tensor)\n","        final_emb = final_emb.cpu().numpy()\n","\n","    print(f\"[INFO] Training completed. Final embeddings shape: {final_emb.shape}\")\n","    return final_emb\n","\n","# ---- 6. Clustering and Evaluation ----\n","def cluster_and_eval_mlp(final_emb, cells, adata, n_clusters=None, leiden_res_list=None):\n","    print(\"[INFO] Running clustering (Leiden + KMeans)...\")\n","\n","    if leiden_res_list is None:\n","        leiden_res_list = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 2.0]\n","\n","    adata.obsm['X_mlp'] = final_emb\n","    sc.pp.neighbors(adata, use_rep='X_mlp', n_neighbors=15)\n","\n","    leiden_silhouettes = []\n","    leiden_label_sets = {}\n","\n","    # Leiden clustering\n","    for res in leiden_res_list:\n","        sc.tl.leiden(adata, resolution=res, key_added=f'mlp_leiden_{res}')\n","        leiden_labels = adata.obs[f'mlp_leiden_{res}'].astype(str).reindex(cells).fillna('NA').tolist()\n","        leiden_labels_num = pd.factorize(leiden_labels)[0]\n","\n","        if len(set(leiden_labels_num)) > 1:\n","            sil = silhouette_score(final_emb, leiden_labels_num)\n","        else:\n","            sil = -1\n","        leiden_silhouettes.append(sil)\n","        leiden_label_sets[res] = leiden_labels\n","        print(f\"[INFO] Leiden res={res:.2f}, clusters={len(set(leiden_labels))}, silhouette={sil:.3f}\")\n","\n","    best_res = leiden_res_list[np.argmax(leiden_silhouettes)]\n","    best_leiden_labels = leiden_label_sets[best_res]\n","    best_leiden_silhouette = max(leiden_silhouettes)\n","\n","    # KMeans clustering\n","    if n_clusters is None:\n","        sil_scores = []\n","        for k in range(3, 15):\n","            labels = KMeans(n_clusters=k, random_state=42).fit_predict(final_emb)\n","            sil_scores.append(silhouette_score(final_emb, labels))\n","        n_clusters = np.argmax(sil_scores) + 3\n","        print(f\"[INFO] Using elbow method: n_clusters={n_clusters}\")\n","    elif n_clusters == \"unique\":\n","        n_clusters = adata.obs['assigned_cluster'].nunique()\n","        print(f\"[INFO] Using unique assigned clusters: n_clusters={n_clusters}\")\n","\n","    kmeans_labels = KMeans(n_clusters=n_clusters, random_state=42).fit_predict(final_emb)\n","    kmeans_silhouette = silhouette_score(final_emb, kmeans_labels)\n","\n","    # Evaluation metrics\n","    if 'assigned_cluster' in adata.obs:\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster']\n","        metrics = {\n","            'Leiden ARI': adjusted_rand_score(true_labels, best_leiden_labels),\n","            'Leiden NMI': normalized_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden AMI': adjusted_mutual_info_score(true_labels, best_leiden_labels),\n","            'Leiden Silhouette': best_leiden_silhouette,\n","            'KMeans ARI': adjusted_rand_score(true_labels, kmeans_labels),\n","            'KMeans NMI': normalized_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans AMI': adjusted_mutual_info_score(true_labels, kmeans_labels),\n","            'KMeans Silhouette': kmeans_silhouette\n","        }\n","        print(\"[INFO] MLP Clustering metrics:\")\n","        for k, v in metrics.items():\n","            print(f\"  {k}: {v:.3f}\")\n","\n","        # Confusion matrix\n","        true_labels = adata.obs.loc[cells, 'assigned_cluster'].astype(str).values\n","        pred_labels = kmeans_labels.astype(str)\n","        cm_df = pd.crosstab(pd.Series(true_labels, name=\"True\"), pd.Series(pred_labels, name=\"Predicted\"))\n","        plt.figure(figsize=(12,8))\n","        sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n","        plt.xlabel(\"Predicted Clusters\")\n","        plt.ylabel(\"True Labels\")\n","        plt.title(\"MLP: Confusion Matrix\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return best_leiden_labels, kmeans_labels, best_res, best_leiden_silhouette, kmeans_silhouette\n","\n","# ---- 7. Visualization ----\n","def visualize_mlp(final_emb, cluster_labels, cells, leiden_labels=None):\n","    print(\"[INFO] Generating visualizations (PCA, UMAP)...\")\n","\n","    # PCA\n","    pca = PCA(n_components=2)\n","    pca_proj = pca.fit_transform(final_emb)\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=pca_proj[:,0], y=pca_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(f'MLP: PCA (Var explained: {pca.explained_variance_ratio_.sum():.2f})')\n","    plt.xlabel(\"PCA1\")\n","    plt.ylabel(\"PCA2\")\n","    plt.legend(title=\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # UMAP\n","    reducer = umap.UMAP(n_components=2, random_state=42)\n","    umap_proj = reducer.fit_transform(final_emb)\n","\n","    if leiden_labels is not None:\n","        plt.figure(figsize=(6,5))\n","        sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=leiden_labels, palette='tab20', s=15)\n","        plt.title(\"MLP: UMAP (Leiden)\")\n","        plt.xlabel(\"UMAP1\")\n","        plt.ylabel(\"UMAP2\")\n","        plt.legend(title=\"Leiden\")\n","        plt.tight_layout()\n","        plt.show()\n","\n","    plt.figure(figsize=(6,5))\n","    sns.scatterplot(x=umap_proj[:,0], y=umap_proj[:,1], hue=cluster_labels, palette='tab20', s=15)\n","    plt.title(\"MLP: UMAP (KMeans)\")\n","    plt.xlabel(\"UMAP1\")\n","    plt.ylabel(\"UMAP2\")\n","    plt.legend(title=\"Cluster\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","# ---- 9. Main Pipeline ----\n","def run_mlp_baseline_pipeline(\n","    expr_csv, metadata_cols=['barcode', 'assigned_cluster'],\n","    regress_vars=None, hvg_n=2000, model_type='autoencoder',\n","    hidden_dims=[512, 256, 128], latent_dim=32, epochs=100, batch_size=256,\n","    learning_rate=0.001, early_stop_patience=15, early_stop_delta=1e-4,\n","    top_n_marker_genes=5, n_clusters=None, rare_cell_thresh=40,\n","    device=\"cuda\"\n","):\n","    print(\"[INFO] === MLP BASELINE (No Graph, No Pathway Information) ===\")\n","    print(\"[INFO] Loading expression matrix...\")\n","\n","    df = pd.read_csv(expr_csv, index_col=0)\n","    metadata = df[metadata_cols]\n","    expr_df = df.drop(columns=metadata_cols)\n","    expr_df = expr_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n","\n","    adata = sc.AnnData(expr_df)\n","    adata.obs = metadata\n","\n","    # Fix duplicate names\n","    if adata.obs_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate cell names. Making unique...\")\n","        adata.obs_names_make_unique()\n","    if adata.var_names.duplicated().any():\n","        print(\"[WARNING] Found duplicate gene names. Making unique...\")\n","        adata.var_names_make_unique()\n","\n","    # Remove rare cell clusters\n","    if 'assigned_cluster' in adata.obs:\n","        counts = adata.obs['assigned_cluster'].value_counts()\n","        keep = counts[counts >= rare_cell_thresh].index\n","        before = adata.n_obs\n","        adata = adata[adata.obs['assigned_cluster'].isin(keep)]\n","        print(f\"[INFO] Removed rare cell types (<{rare_cell_thresh} cells). {before} → {adata.n_obs} cells kept.\")\n","\n","    # Preprocess\n","    adata = preprocess_sctransform(adata, regress_vars=regress_vars, hvg_n=hvg_n)\n","\n","    cells = list(adata.obs_names)\n","    data = adata.X\n","\n","    # Train MLP\n","    final_emb = train_mlp_autoencoder(\n","        data, model_type=model_type, hidden_dims=hidden_dims, latent_dim=latent_dim,\n","        epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,\n","        early_stop_patience=early_stop_patience, early_stop_delta=early_stop_delta,\n","        device=device\n","    )\n","\n","    # Clustering and evaluation\n","    leiden_labels, kmeans_labels, best_res, leiden_sil, kmeans_sil = cluster_and_eval_mlp(\n","        final_emb, cells, adata, n_clusters=n_clusters\n","    )\n","\n","    # Marker gene analysis\n","    print(\"[INFO] Running marker gene analysis...\")\n","    adata.obs[\"cluster\"] = kmeans_labels.astype(str)\n","    sc.tl.rank_genes_groups(adata, groupby=\"cluster\", method=\"wilcoxon\", use_raw=False)\n","    sc.pl.rank_genes_groups(adata, n_genes=top_n_marker_genes, sharey=False)\n","\n","    markers_df = sc.get.rank_genes_groups_df(adata, group=None)\n","    for c in sorted(adata.obs[\"cluster\"].unique()):\n","        top_genes = markers_df[markers_df[\"group\"] == c].head(top_n_marker_genes)\n","        print(f\"\\nMLP Cluster {c} top marker genes:\")\n","        for _, row in top_genes.iterrows():\n","            sig = (\n","                \"***\" if row[\"pvals_adj\"] < 0.001 else\n","                \"**\" if row[\"pvals_adj\"] < 0.01 else\n","                \"*\" if row[\"pvals_adj\"] < 0.05 else \"\"\n","            )\n","            print(f\"  {row['names']}: logFC={row['logfoldchanges']:.2f}, adj.p={row['pvals_adj']:.3g} {sig}\")\n","\n","    # Visualizations\n","    visualize_mlp(final_emb, kmeans_labels, cells, leiden_labels)\n","\n","\n","# Example usage:\n","run_mlp_baseline_pipeline(\n","    'your_scRNA seq_csv_dataset',\n","    model_type=None, epochs=150, n_clusters=\"unique\"\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"19Cdps3c5FNC9aTdx45RPL9U9LHGPScKf","timestamp":1762400457497}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}